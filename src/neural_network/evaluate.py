"""
Script de evaluare pentru re»õeaua neuronalƒÉ Text-to-Blender.
==============================================================

Acest script:
1. √éncarcƒÉ modelul antrenat
2. EvalueazƒÉ pe setul de test
3. GenereazƒÉ raport detaliat per clasƒÉ
4. CalculeazƒÉ metrici suplimentare

Rulare:
    python -m src.neural_network.evaluate
"""

import os
import sys
import json
import pickle
from pathlib import Path

import numpy as np
import torch
import pandas as pd
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    f1_score, precision_score, recall_score,
    top_k_accuracy_score
)
import yaml
import matplotlib.pyplot as plt
import seaborn as sns

# AdƒÉugare path pentru import
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.neural_network.model import NeuralNetwork


def load_model(models_dir: str, config: dict, input_size: int, output_size: int) -> NeuralNetwork:
    """√éncarcƒÉ modelul antrenat."""
    model = NeuralNetwork(
        input_size=input_size,
        hidden_layers=config.get('model', {}).get('hidden_layers', [128, 64, 32]),
        output_size=output_size,
        activation=config.get('model', {}).get('activation', 'relu'),
        output_activation='softmax',
        dropout=config.get('model', {}).get('dropout', 0.2)
    )
    
    model_path = Path(models_dir) / "trained_model.pt"
    
    if not model_path.exists():
        raise FileNotFoundError(f"Modelul nu existƒÉ: {model_path}")
    
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"‚úÖ Model √ÆncƒÉrcat din: {model_path}")
    return model


def load_test_data(data_dir: str) -> tuple:
    """√éncarcƒÉ datele de test."""
    data_path = Path(data_dir) / "test"
    
    X_test = np.load(data_path / "X_test.npy")
    y_test = np.load(data_path / "y_test.npy")
    
    print(f"‚úÖ Date test: {X_test.shape[0]} samples")
    return X_test, y_test


def load_params(config_dir: str) -> dict:
    """√éncarcƒÉ parametrii de preprocesare."""
    params_path = Path(config_dir) / "preprocessing_params.pkl"
    with open(params_path, 'rb') as f:
        return pickle.load(f)


def load_config(config_dir: str) -> dict:
    """√éncarcƒÉ configura»õia modelului."""
    config_path = Path(config_dir) / "model_config.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def predict(model: NeuralNetwork, X: np.ndarray) -> tuple:
    """GenereazƒÉ predic»õii."""
    model.eval()
    X_tensor = torch.FloatTensor(X)
    
    with torch.no_grad():
        outputs = model(X_tensor)
        probabilities = torch.softmax(outputs, dim=1).numpy()
        predictions = np.argmax(probabilities, axis=1)
    
    return predictions, probabilities


def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray, probabilities: np.ndarray) -> dict:
    """CalculeazƒÉ toate metricile de evaluare."""
    metrics = {
        'accuracy': np.mean(y_true == y_pred),
        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),
        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),
        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),
        'precision_weighted': precision_score(y_true, y_pred, average='weighted', zero_division=0),
        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),
        'recall_weighted': recall_score(y_true, y_pred, average='weighted', zero_division=0),
    }
    
    # Top-k accuracy cu toate clasele
    num_classes = probabilities.shape[1]
    all_labels = list(range(num_classes))
    
    for k in [3, 5]:
        if k <= num_classes:
            try:
                metrics[f'top_{k}_accuracy'] = top_k_accuracy_score(
                    y_true, probabilities, k=k, labels=all_labels
                )
            except Exception:
                # Fallback dacƒÉ top-k nu func»õioneazƒÉ
                pass
    
    return metrics


def generate_per_class_report(
    y_true: np.ndarray, 
    y_pred: np.ndarray, 
    idx_to_intent: dict,
    results_dir: str
):
    """GenereazƒÉ raport detaliat per clasƒÉ."""
    results_path = Path(results_dir)
    
    # GƒÉse»ôte clasele prezente √Æn date
    unique_labels = np.unique(np.concatenate([y_true, y_pred]))
    target_names = [idx_to_intent.get(int(i), f'Class_{i}') for i in unique_labels]
    
    # Classification report
    report = classification_report(
        y_true, y_pred, 
        labels=unique_labels,
        target_names=target_names,
        output_dict=True,
        zero_division=0
    )
    
    # Conversie √Æn DataFrame
    report_df = pd.DataFrame(report).transpose()
    
    # Salvare CSV
    report_path = results_path / "per_class_metrics.csv"
    report_df.to_csv(report_path)
    print(f"üìä Raport per clasƒÉ salvat: {report_path}")
    
    return report_df


def analyze_errors(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    idx_to_intent: dict,
    results_dir: str
):
    """AnalizeazƒÉ erorile de clasificare."""
    results_path = Path(results_dir)
    
    # GƒÉsire erori
    errors_mask = y_true != y_pred
    error_indices = np.where(errors_mask)[0]
    
    error_analysis = []
    for idx in error_indices:
        error_analysis.append({
            'sample_idx': int(idx),
            'true_label': idx_to_intent.get(int(y_true[idx]), f'Class_{y_true[idx]}'),
            'predicted_label': idx_to_intent.get(int(y_pred[idx]), f'Class_{y_pred[idx]}')
        })
    
    # Salvare
    errors_df = pd.DataFrame(error_analysis)
    errors_path = results_path / "error_analysis.csv"
    errors_df.to_csv(errors_path, index=False)
    print(f"üìä AnalizƒÉ erori salvatƒÉ: {errors_path}")
    
    # Statistici erori
    print(f"\nüîç AnalizƒÉ erori:")
    print(f"   Total erori: {len(error_analysis)} din {len(y_true)} ({len(error_analysis)/len(y_true)*100:.2f}%)")
    
    # Cele mai frecvente confuzii
    if len(errors_df) > 0:
        confusion_pairs = errors_df.groupby(['true_label', 'predicted_label']).size().sort_values(ascending=False)
        print(f"\n   Top 5 confuzii:")
        for i, ((true_l, pred_l), count) in enumerate(confusion_pairs.head(5).items()):
            print(f"   {i+1}. {true_l} ‚Üí {pred_l}: {count} erori")


def plot_class_distribution(y_test: np.ndarray, idx_to_intent: dict, results_dir: str):
    """GenereazƒÉ grafic distribu»õie clase √Æn test."""
    results_path = Path(results_dir)
    
    # Contorizare
    unique, counts = np.unique(y_test, return_counts=True)
    class_names = [idx_to_intent.get(i, f'Class_{i}')[:20] for i in unique]
    
    # Sortare descrescƒÉtoare
    sorted_idx = np.argsort(counts)[::-1][:20]  # Top 20
    
    # Plot
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.bar(range(len(sorted_idx)), counts[sorted_idx], color='steelblue')
    ax.set_xticks(range(len(sorted_idx)))
    ax.set_xticklabels([class_names[i] for i in sorted_idx], rotation=45, ha='right')
    ax.set_xlabel('ClasƒÉ')
    ax.set_ylabel('NumƒÉr samples')
    ax.set_title('Distribu»õia claselor √Æn setul de test (Top 20)')
    
    plt.tight_layout()
    plt.savefig(results_path / "test_class_distribution.png", dpi=150)
    plt.close()
    print(f"üìä Distribu»õie clase salvatƒÉ: {results_path / 'test_class_distribution.png'}")


def main():
    """Func»õia principalƒÉ de evaluare."""
    print("=" * 60)
    print("üìä Text-to-Blender Model Evaluation")
    print("=" * 60)
    
    # Paths
    project_root = Path(__file__).parent.parent.parent
    data_dir = project_root / "data"
    config_dir = project_root / "config"
    models_dir = project_root / "models"
    results_dir = project_root / "results"
    
    # 1. √éncƒÉrcare parametri »ôi config
    params = load_params(str(config_dir))
    config = load_config(str(config_dir))
    
    # 2. √éncƒÉrcare date test
    X_test, y_test = load_test_data(str(data_dir))
    
    # 3. √éncƒÉrcare model
    model = load_model(
        str(models_dir), config,
        input_size=X_test.shape[1],
        output_size=params['num_classes']
    )
    
    # 4. Predic»õii
    print("\nüîÆ Generare predic»õii...")
    predictions, probabilities = predict(model, X_test)
    
    # 5. Calculare metrici
    print("\nüìà Calculare metrici...")
    metrics = calculate_metrics(y_test, predictions, probabilities)
    
    print(f"\n{'='*40}")
    print("üìä REZULTATE EVALUARE")
    print(f"{'='*40}")
    print(f"  Accuracy:           {metrics['accuracy']*100:.2f}%")
    print(f"  F1 Score (macro):   {metrics['f1_macro']:.4f}")
    print(f"  F1 Score (weighted):{metrics['f1_weighted']:.4f}")
    print(f"  Precision (macro):  {metrics['precision_macro']:.4f}")
    print(f"  Recall (macro):     {metrics['recall_macro']:.4f}")
    if 'top_3_accuracy' in metrics:
        print(f"  Top-3 Accuracy:     {metrics['top_3_accuracy']*100:.2f}%")
    if 'top_5_accuracy' in metrics:
        print(f"  Top-5 Accuracy:     {metrics['top_5_accuracy']*100:.2f}%")
    print(f"{'='*40}")
    
    # Verificare obiective
    print(f"\nüéØ Verificare obiective Etapa 5:")
    acc_ok = metrics['accuracy'] >= 0.65
    f1_ok = metrics['f1_macro'] >= 0.60
    print(f"   Accuracy ‚â• 65%: {'‚úÖ DA' if acc_ok else '‚ùå NU'} ({metrics['accuracy']*100:.2f}%)")
    print(f"   F1 ‚â• 0.60:      {'‚úÖ DA' if f1_ok else '‚ùå NU'} ({metrics['f1_macro']:.4f})")
    
    # 6. Raport per clasƒÉ
    generate_per_class_report(
        y_test, predictions, params['idx_to_intent'], str(results_dir)
    )
    
    # 7. AnalizƒÉ erori
    analyze_errors(
        y_test, predictions, params['idx_to_intent'], str(results_dir)
    )
    
    # 8. Distribu»õie clase
    plot_class_distribution(y_test, params['idx_to_intent'], str(results_dir))
    
    # 9. Salvare metrici complete
    eval_metrics_path = results_dir / "evaluation_metrics.json"
    with open(eval_metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2)
    print(f"\nüìä Metrici complete salvate: {eval_metrics_path}")
    
    print("\n" + "=" * 60)
    print("‚úÖ Evaluare completƒÉ!")
    print("=" * 60)
    
    return metrics


if __name__ == "__main__":
    main()
