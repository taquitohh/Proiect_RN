# ğŸ§  Neural Network Module

## Descriere
Acest modul conÈ›ine **arhitectura reÈ›elei neuronale** pentru clasificarea intenÈ›iilor din comenzi text.

## Structura
```
neural_network/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ model.py           # DefiniÈ›ie model È™i trainer
â””â”€â”€ README.md          # Acest fiÈ™ier
```

## Arhitectura Modelului

### Clasa `NeuralNetwork`
ReÈ›ea neuronalÄƒ feed-forward cu arhitecturÄƒ configurabilÄƒ:

```python
NeuralNetwork(
    input_size=768,        # Dimensiune embedding text
    hidden_layers=[256, 128, 64],  # Straturi ascunse
    output_size=20,        # NumÄƒr de intenÈ›ii
    activation='relu',     # FuncÈ›ie activare
    dropout=0.2            # Regularizare
)
```

### Justificarea arhitecturii
1. **Input 768 dimensiuni** - Compatibil cu embeddings de la modele pre-antrenate (BERT, etc.)
2. **3 straturi ascunse** - Suficient pentru clasificare text de complexitate medie
3. **Reducere progresivÄƒ (256â†’128â†’64)** - Extragere features ierarhicÄƒ
4. **ReLU activation** - Standard pentru reÈ›ele feed-forward, evitÄƒ vanishing gradients
5. **Dropout 20%** - Previne overfitting pe dataset mic

### FuncÈ›ii principale
- `forward(x)` - Forward pass prin reÈ›ea
- `predict(x)` - PredicÈ›ie cu softmax pentru clasificare
- `save_model(path)` - Salvare weights
- `load_model(path)` - ÃncÄƒrcare weights

### Clasa `NeuralNetworkTrainer`
GestioneazÄƒ antrenarea modelului:
- Loss function: CrossEntropyLoss
- Optimizer: Adam (lr=0.001)
- Early stopping configurabil

## Status actual (Etapa 4)
- âœ… ArhitecturÄƒ definitÄƒ È™i compilatÄƒ
- âœ… Model poate fi salvat/Ã®ncÄƒrcat
- â³ Antrenare preliminarÄƒ (weights iniÈ›ializaÈ›i random)
- â³ Optimizare hiperparametri (Etapa 5)

## Comenzi de test
```bash
# Verificare cÄƒ modelul se poate instanÈ›ia
python -c "from neural_network.model import NeuralNetwork; m = NeuralNetwork(100, [64, 32], 10); print(m)"

# Test forward pass
python -c "
import torch
from neural_network.model import NeuralNetwork
model = NeuralNetwork(100, [64, 32], 10)
x = torch.randn(1, 100)
print('Output shape:', model(x).shape)
"
```

## Configurare (config/model_config.yaml)
```yaml
model:
  input_size: 768
  hidden_layers: [256, 128, 64]
  output_size: 20
  activation: relu
  dropout: 0.2
  
training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  early_stopping_patience: 10
```
